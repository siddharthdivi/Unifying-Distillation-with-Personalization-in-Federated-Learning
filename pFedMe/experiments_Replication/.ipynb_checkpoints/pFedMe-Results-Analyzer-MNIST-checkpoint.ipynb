{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to help read the h5 files.\n",
    "def simple_read_data(fileName):\n",
    "    print(fileName)\n",
    "    hf = h5py.File('{}.h5'.format(fileName), 'r')\n",
    "    \n",
    "    # We'll return a dictionary object. \n",
    "    results = {}\n",
    "    \n",
    "    results['rs_glob_acc'] = np.array(hf.get('rs_glob_acc')[:])\n",
    "    results['rs_train_acc'] = np.array(hf.get('rs_train_acc')[:])\n",
    "    results['rs_train_loss'] = np.array(hf.get('rs_train_loss')[:])\n",
    "    \n",
    "    # 3D array: Read as [number of times, number of epochs, number of users].\n",
    "    results['perUserAccs'] = np.array(hf.get('perUserAccs'))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global directory path.\n",
    "directoryPath = '/home/adgdri/pFedMe/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasplit-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = [\n",
    "    # TODO: Enter the filenames of the experimental results over the different runs.\n",
    "]\n",
    "\n",
    "\n",
    "# Get the number of users.\n",
    "numUsers = int(fileNames[0].split('u')[0].split('_')[-1])\n",
    "\n",
    "avgPersAcc = []\n",
    "perUserAcc = np.zeros((1, numUsers))\n",
    "\n",
    "for fileName in fileNames:\n",
    "    ob = simple_read_data(directoryPath + fileName)\n",
    "    avgPersAcc.append( ob['rs_glob_acc'][-1] )\n",
    "    # Take the per user accuracy from the last epoch.\n",
    "    perUserAcc += ob['perUserAccs'][:,-1, :]\n",
    "\n",
    "# Average out over the different runs.\n",
    "perUserAcc /= len(fileNames)\n",
    "\n",
    "\n",
    "print ('----------------------------------------')\n",
    "\n",
    "print ('\\n Average accuracies across all the users over different runs : %s' % avgPersAcc)\n",
    "\n",
    "print ('\\n Average accuracy across all the different runs : %f.' % np.mean(avgPersAcc) )\n",
    "\n",
    "print ('\\n Average per user accuracy averaged over different runs: \\n %s.' % np.round(perUserAcc.T, 4))\n",
    "\n",
    "print ('\\n Per-user averaged accuracy: \\n %f.' % np.mean(np.round(perUserAcc.T, 4))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasplit-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = [\n",
    "    # TODO: Enter the filenames of the experimental results over the different runs.\n",
    "]\n",
    "\n",
    "\n",
    "# Get the number of users.\n",
    "numUsers = int(fileNames[0].split('u')[0].split('_')[-1])\n",
    "\n",
    "avgPersAcc = []\n",
    "perUserAcc = np.zeros((1, numUsers))\n",
    "\n",
    "for fileName in fileNames:\n",
    "    ob = simple_read_data(directoryPath + fileName)\n",
    "    avgPersAcc.append( ob['rs_glob_acc'][-1] )\n",
    "    # Take the per user accuracy from the last epoch.\n",
    "    perUserAcc += ob['perUserAccs'][:,-1, :]\n",
    "\n",
    "# Average out over the different runs.\n",
    "perUserAcc /= len(fileNames)\n",
    "\n",
    "\n",
    "print ('----------------------------------------')\n",
    "\n",
    "print ('\\n Average accuracies across all the users over different runs : %s' % avgPersAcc)\n",
    "\n",
    "print ('\\n Average accuracy across all the different runs : %f.' % np.mean(avgPersAcc) )\n",
    "\n",
    "print ('\\n Average per user accuracy averaged over different runs: \\n %s.' % np.round(perUserAcc.T, 4))\n",
    "\n",
    "print ('\\n Per-user averaged accuracy: \\n %f.' % np.mean(np.round(perUserAcc.T, 4))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasplit-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = [\n",
    "    # TODO: Enter the filenames of the experimental results over the different runs.\n",
    "]\n",
    "\n",
    "\n",
    "# Get the number of users.\n",
    "numUsers = int(fileNames[0].split('u')[0].split('_')[-1])\n",
    "\n",
    "avgPersAcc = []\n",
    "perUserAcc = np.zeros((1, numUsers))\n",
    "\n",
    "for fileName in fileNames:\n",
    "    ob = simple_read_data(directoryPath + fileName)\n",
    "    avgPersAcc.append( ob['rs_glob_acc'][-1] )\n",
    "    # Take the per user accuracy from the last epoch.\n",
    "    perUserAcc += ob['perUserAccs'][:,-1, :]\n",
    "\n",
    "# Average out over the different runs.\n",
    "perUserAcc /= len(fileNames)\n",
    "\n",
    "\n",
    "print ('----------------------------------------')\n",
    "\n",
    "print ('\\n Average accuracies across all the users over different runs : %s' % avgPersAcc)\n",
    "\n",
    "print ('\\n Average accuracy across all the different runs : %f.' % np.mean(avgPersAcc) )\n",
    "\n",
    "print ('\\n Average per user accuracy averaged over different runs: \\n %s.' % np.round(perUserAcc.T, 4))\n",
    "\n",
    "print ('\\n Per-user averaged accuracy: \\n %f.' % np.mean(np.round(perUserAcc.T, 4))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
