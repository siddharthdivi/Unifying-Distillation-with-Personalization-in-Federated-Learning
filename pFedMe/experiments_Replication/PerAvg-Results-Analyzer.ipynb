{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is the Result Analyzer notebook for the Per-FedAvg on the CIFAR-10 dataset for all three data-splits: \n",
    "(DS-1, DS-2, DS-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to help read the h5 files.\n",
    "def simple_read_data(fileName):\n",
    "    print(fileName)\n",
    "    hf = h5py.File('{}.h5'.format(fileName), 'r')\n",
    "    \n",
    "    # We'll return a dictionary object. \n",
    "    results = {}\n",
    "    \n",
    "    results['rs_glob_acc'] = np.array(hf.get('rs_glob_acc')[:])\n",
    "    results['rs_train_acc'] = np.array(hf.get('rs_train_acc')[:])\n",
    "    results['rs_train_loss'] = np.array(hf.get('rs_train_loss')[:])\n",
    "    \n",
    "    # 3D array: Read as [number of times, number of epochs, number of users].\n",
    "    results['perUserAccs'] = np.array(hf.get('perUserAccs'))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasplit-1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enter the directoryPath of the 'results/' folder which is where all the result files of all experiments run would be stored. Right now it is linked to 'results_oldRuns' because this is where all the old experimental results files are stored. When you run your experiments, your results will be stored in 'results/' and hence you'll replace the following path with : '~/pFedMe/results/'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global directory path.\n",
    "directoryPath = '/home/adgdri/pFedMe/results_oldRuns/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you run your set of experiments, then all you have to do to analyze those set of files is that you copy the names of all the files into the list variable 'fileNames'. Notice how the entries of the 'fileNames' variable only differ in the roundNum_x subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adgdri/pFedMe/results_oldRuns/Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_0_globalIters_800_dataSplit_1\n",
      "/home/adgdri/pFedMe/results_oldRuns/Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_1_globalIters_800_dataSplit_1\n",
      "/home/adgdri/pFedMe/results_oldRuns/Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_2_globalIters_800_dataSplit_1\n",
      "/home/adgdri/pFedMe/results_oldRuns/Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_3_globalIters_800_dataSplit_1\n",
      "/home/adgdri/pFedMe/results_oldRuns/Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_4_globalIters_800_dataSplit_1\n",
      "----------------------------------------\n",
      "\n",
      " Average accuracies across all the users over different runs : [0.7304, 0.6293, 0.7196, 0.6333, 0.6337]\n",
      "\n",
      " Average accuracy across all the different runs : 0.669260.\n",
      "\n",
      " Average per user accuracy averaged over different runs: \n",
      " [[0.6916]\n",
      " [0.6504]\n",
      " [0.6786]\n",
      " [0.6718]\n",
      " [0.6576]\n",
      " [0.6268]\n",
      " [0.5824]\n",
      " [0.6428]\n",
      " [0.7246]\n",
      " [0.766 ]].\n",
      "\n",
      " Average per user across all different runs: \n",
      " 0.66926.\n"
     ]
    }
   ],
   "source": [
    "fileNames = [\n",
    "    'Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_0_globalIters_800_dataSplit_1',\n",
    "    'Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_1_globalIters_800_dataSplit_1',\n",
    "    'Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_2_globalIters_800_dataSplit_1',\n",
    "    'Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_3_globalIters_800_dataSplit_1',\n",
    "    'Cifar10_PerAvg_p_0.001_0.001_15_10u_128b_20_avg_roundNum_4_globalIters_800_dataSplit_1',\n",
    "]\n",
    "\n",
    "\n",
    "# Get the number of users.\n",
    "numUsers = int(fileNames[0].split('u')[0].split('_')[-1])\n",
    "\n",
    "avgPersAcc = []\n",
    "perUserAcc = np.zeros((1, numUsers))\n",
    "\n",
    "for fileName in fileNames:\n",
    "    ob = simple_read_data(directoryPath + fileName)\n",
    "    avgPersAcc.append( ob['rs_glob_acc'][-1] )\n",
    "    # Take the per user accuracy from the last epoch.\n",
    "    perUserAcc += ob['perUserAccs'][:,-1,:]\n",
    "\n",
    "# Average out over the different runs.\n",
    "perUserAcc /= len(fileNames)\n",
    "\n",
    "\n",
    "print ('----------------------------------------')\n",
    "\n",
    "print ('\\n Average accuracies across all the users over different runs : %s' % avgPersAcc)\n",
    "\n",
    "print ('\\n Average accuracy across all the different runs : %f.' % np.mean(avgPersAcc) )\n",
    "\n",
    "print ('\\n Average per user accuracy averaged over different runs: \\n %s.' % np.round(perUserAcc.T, 4))\n",
    "\n",
    "print ('\\n Average per user across all different runs: \\n %s.' % np.mean(np.round(perUserAcc.T, 4)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasplit-2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enter the directoryPath of the 'results/' folder which is where all the result files of all experiments run would be stored. Right now it is linked to 'results_oldRuns' because this is where all the old experimental results files are stored. When you run your experiments, your results will be stored in 'results/' and hence you'll replace the following path with : '~/pFedMe/results/'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global directory path.\n",
    "directoryPath = '/home/adgdri/pFedMe/results_oldRuns/perFedTempResults/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you run your set of experiments, then all you have to do to analyze those set of files is that you copy the names of all the files into the list variable 'fileNames'. Notice how the entries of the 'fileNames' variable only differ in the roundNum_x subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adgdri/pFedMe/results_oldRuns/perFedTempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_0_globalIters_800_dataSplit_2\n",
      "/home/adgdri/pFedMe/results_oldRuns/perFedTempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_1_globalIters_800_dataSplit_2\n",
      "/home/adgdri/pFedMe/results_oldRuns/perFedTempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_2_globalIters_800_dataSplit_2\n",
      "/home/adgdri/pFedMe/results_oldRuns/perFedTempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_3_globalIters_800_dataSplit_2\n",
      "/home/adgdri/pFedMe/results_oldRuns/perFedTempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_4_globalIters_800_dataSplit_2\n",
      "----------------------------------------\n",
      "\n",
      " Average accuracies across all the users over different runs : [0.5807257822653205, 0.5977206837948615, 0.5789842031593682, 0.5450364890532841, 0.5604318704388683]\n",
      "\n",
      " Average accuracy across all the different runs : 0.572580.\n",
      "\n",
      " Average per user accuracy averaged over different runs: \n",
      " [[0.5818]\n",
      " [0.5609]\n",
      " [0.5719]\n",
      " [0.5883]\n",
      " [0.5935]\n",
      " [0.5927]\n",
      " [0.5768]\n",
      " [0.5802]\n",
      " [0.5575]\n",
      " [0.5533]].\n",
      "\n",
      " Average per user across all different runs: \n",
      " 0.57569.\n"
     ]
    }
   ],
   "source": [
    "fileNames = [\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_0_globalIters_800_dataSplit_2',\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_1_globalIters_800_dataSplit_2',\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_2_globalIters_800_dataSplit_2',\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_3_globalIters_800_dataSplit_2',\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_4_globalIters_800_dataSplit_2',\n",
    "]\n",
    "\n",
    "\n",
    "# Get the number of users.\n",
    "numUsers = int(fileNames[0].split('u')[0].split('_')[-1])\n",
    "\n",
    "avgPersAcc = []\n",
    "perUserAcc = np.zeros((1, numUsers))\n",
    "\n",
    "for fileName in fileNames:\n",
    "    ob = simple_read_data(directoryPath + fileName)\n",
    "    avgPersAcc.append( ob['rs_glob_acc'][-1] )\n",
    "    # Take the per user accuracy from the last epoch.\n",
    "    perUserAcc += ob['perUserAccs'][:,-1,:]\n",
    "\n",
    "# Average out over the different runs.\n",
    "perUserAcc /= len(fileNames)\n",
    "\n",
    "\n",
    "print ('----------------------------------------')\n",
    "\n",
    "print ('\\n Average accuracies across all the users over different runs : %s' % avgPersAcc)\n",
    "\n",
    "print ('\\n Average accuracy across all the different runs : %f.' % np.mean(avgPersAcc) )\n",
    "\n",
    "print ('\\n Average per user accuracy averaged over different runs: \\n %s.' % np.round(perUserAcc.T, 4))\n",
    "\n",
    "print ('\\n Average per user across all different runs: \\n %s.' % np.mean(np.round(perUserAcc.T, 4)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasplit-3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enter the directoryPath of the 'results/' folder which is where all the result files of all experiments run would be stored. Right now it is linked to 'results_oldRuns' because this is where all the old experimental results files are stored. When you run your experiments, your results will be stored in 'results/' and hence you'll replace the following path with : '~/pFedMe/results/'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global directory path.\n",
    "directoryPath = '/home/adgdri/pFedMe/results_oldRuns/tempResults/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you run your set of experiments, then all you have to do to analyze those set of files is that you copy the names of all the files into the list variable 'fileNames'. Notice how the entries of the 'fileNames' variable only differ in the roundNum_x subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adgdri/pFedMe/results_oldRuns/tempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_0_globalIters_1000_dataSplit_3\n",
      "/home/adgdri/pFedMe/results_oldRuns/tempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_1_globalIters_1000_dataSplit_3\n",
      "/home/adgdri/pFedMe/results_oldRuns/tempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_2_globalIters_1000_dataSplit_3\n",
      "/home/adgdri/pFedMe/results_oldRuns/tempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_3_globalIters_1000_dataSplit_3\n",
      "/home/adgdri/pFedMe/results_oldRuns/tempResults/Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_4_globalIters_1000_dataSplit_3\n",
      "----------------------------------------\n",
      "\n",
      " Average accuracies across all the users over different runs : [0.7798149118211979, 0.7745765671381177, 0.7606076479832373, 0.7667190501134975, 0.7604330364938013]\n",
      "\n",
      " Average accuracy across all the different runs : 0.768430.\n",
      "\n",
      " Average per user accuracy averaged over different runs: \n",
      " [[0.9247]\n",
      " [0.7373]\n",
      " [0.6464]\n",
      " [0.7703]\n",
      " [0.825 ]\n",
      " [0.7793]\n",
      " [0.8913]\n",
      " [0.8373]\n",
      " [0.641 ]\n",
      " [0.7266]].\n",
      "\n",
      " Average per user across all different runs: \n",
      " 0.7779200000000001.\n"
     ]
    }
   ],
   "source": [
    "fileNames = [\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_0_globalIters_1000_dataSplit_3',\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_1_globalIters_1000_dataSplit_3',\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_2_globalIters_1000_dataSplit_3',\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_3_globalIters_1000_dataSplit_3',\n",
    "    'Cifar10_PerAvg_p_0.01_0.0025_15_10u_128b_20_avg_roundNum_4_globalIters_1000_dataSplit_3',\n",
    "]\n",
    "\n",
    "\n",
    "# Get the number of users.\n",
    "numUsers = int(fileNames[0].split('u')[0].split('_')[-1])\n",
    "\n",
    "avgPersAcc = []\n",
    "perUserAcc = np.zeros((1, numUsers))\n",
    "\n",
    "for fileName in fileNames:\n",
    "    ob = simple_read_data(directoryPath + fileName)\n",
    "    avgPersAcc.append( ob['rs_glob_acc'][-1] )\n",
    "    # Take the per user accuracy from the last epoch.\n",
    "    perUserAcc += ob['perUserAccs'][:,-1,:]\n",
    "\n",
    "# Average out over the different runs.\n",
    "perUserAcc /= len(fileNames)\n",
    "\n",
    "\n",
    "print ('----------------------------------------')\n",
    "\n",
    "print ('\\n Average accuracies across all the users over different runs : %s' % avgPersAcc)\n",
    "\n",
    "print ('\\n Average accuracy across all the different runs : %f.' % np.mean(avgPersAcc) )\n",
    "\n",
    "print ('\\n Average per user accuracy averaged over different runs: \\n %s.' % np.round(perUserAcc.T, 4))\n",
    "\n",
    "print ('\\n Average per user across all different runs: \\n %s.' % np.mean(np.round(perUserAcc.T, 4)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
